name: sparc_hifigan_librittsr
model_configs:
  generator_configs:
    in_channels: 14                       # Number of input channels.
    out_channels: 1                       # Number of output channels.
    channels: 512                         # Number of initial channels.
    kernel_size: 7                        # was 7. Kernel size of initial and final conv layers.
    upsample_scales: [8, 5, 4, 2]         # Upsampling scales.
    upsample_kernel_sizes: [16, 10, 8, 4] # Kernel size for upsampling layers.
    resblock_kernel_sizes: [3, 7, 11]     # Kernel size for residual blocks.
    resblock_dilations:                   # Dilations for residual blocks.
        - [1, 3, 5]
        - [1, 3, 5]
        - [1, 3, 5]
    use_additional_convs: true            # Whether to use additional conv layer in residual blocks.
    bias: true                            # Whether to use bias parameter in conv.
    nonlinear_activation: "LeakyReLU"     # Nonlinear activation type.
    nonlinear_activation_params:          # Nonlinear activation paramters.
      negative_slope: 0.1
    use_weight_norm: true                 # Whether to apply weight normalization.
    pitch_offset: 50                      # normalized pitch: (pitch-pitch_offset)*pitch_rescale
    pitch_rescale: 0.01
    pitch_axis: 12                        # axis position of pitch in in_channels
    spk_emb_size: 64                      # speaker embedding size                    
  spk_layer_configs: 
    spk_ft_size: 1024                     # input acoustic features size           
    spk_emb_size: 64                      # output speaker embedding size
  discriminator_configs:
    scales: 3                              # Number of multi-scale discriminator.
    scale_downsample_pooling: "AvgPool1d"  # Pooling operation for scale discriminator.
    scale_downsample_pooling_params:
      kernel_size: 4                     # Pooling kernel size.
      stride: 2                          # Pooling stride.
      padding: 2                         # Padding size.
    scale_discriminator_params:
      in_channels: 1                     # Number of input channels.
      out_channels: 1                    # Number of output channels.
      kernel_sizes: [15, 41, 5, 3]       # List of kernel sizes.
      channels: 128                      # Initial number of channels.
      max_downsample_channels: 1024      # Maximum number of channels in downsampling conv layers.
      max_groups: 16                     # Maximum number of groups in downsampling conv layers.
      bias: true
      downsample_scales: [4, 4, 4, 4, 1] # Downsampling scales.
      nonlinear_activation: "LeakyReLU"  # Nonlinear activation.
      nonlinear_activation_params:
        negative_slope: 0.1
      use_weight_norm: true              # Whether to apply weight normalization.
      use_spectral_norm: false           # Whether to apply spectral normalization.
    follow_official_norm: true             # Whether to follow the official norm setting.
    periods: [2, 3, 5, 7, 11]              # List of period for multi-period discriminator.
    period_discriminator_params:
      in_channels: 1                     # Number of input channels.
      out_channels: 1                    # Number of output channels.
      kernel_sizes: [5, 3]               # List of kernel sizes.
      channels: 32                       # Initial number of channels.
      downsample_scales: [3, 3, 3, 3, 1] # Downsampling scales.
      max_downsample_channels: 1024      # Maximum number of channels in downsampling conv layers.
      bias: true                         # Whether to use bias parameter in conv layer."
      nonlinear_activation: "LeakyReLU"  # Nonlinear activation.
      nonlinear_activation_params:       # Nonlinear activation paramters.
        negative_slope: 0.1
      use_weight_norm: true              # Whether to apply weight normalization.
      use_spectral_norm: false           # Whether to apply spectral normalization.
      period: null
  loss_configs:
    mel_loss_configs:
      fs: 16000
      fft_size: 1024
      hop_size: 160 # was 110
      win_length: 1024
      window: "hann"
      num_mels: 80
      fmin: 0
      fmax: 8000
      log_base: null
    feature_match_loss_configs:
      average_by_discriminators: false # Whether to average loss by #discriminators.
      average_by_layers: false         # Whether to average loss by #layers in each discriminator.
      include_final_outputs: false     # Whether to include final outputs in feat match loss calculation.
    generator_adv_loss_configs:
      average_by_discriminators: false
    discriminator_adv_loss_configs:
      average_by_discriminators: false
gen_loss_coef:
  mel_loss: 45.0
  feat_match_loss: 2
  gen_adv_loss: 1
disc_loss_coef:
  disc_adv_loss: 1
gen_lr_configs:
  optimizer:
    lr: 1.0e-4
    betas: [0.5, 0.9]
    weight_decay: 0.0
  scheduler:
    gamma: 0.5
    milestones:
    - 80000
    - 160000
    - 240000
    - 320000
disc_lr_configs:
  optimizer:
    lr: 1.0e-4
    betas: [0.5, 0.9]
    weight_decay: 0.0
  scheduler:
    gamma: 0.5
    milestones:
    - 80000
    - 160000
    - 240000
    - 320000   
data:
  train_datasets:
    librittsr:
      file: files/librittsr/train.txt
      wav_dir: data/LibriTTS_R/wavs
      art_dir: data/LibriTTS_R/sparc/art
      spk_dir: data/LibriTTS_R/sparc/spk
  val_datasets:
    librittsr:
      file: files/librittsr/dev.txt
      wav_dir: data/LibriTTS_R/wavs
      art_dir: data/LibriTTS_R/sparc/art
      spk_dir: data/LibriTTS_R/sparc/spk
  batch_size: 64
  val_batch_size: 32
  sample_len: 5120
  num_workers: 16
checkpoint_epoch: 1
max_steps: 1000000
limit_val_batches: 100
speech_model_ckpt: null
model_ckpt: null
resume_ckpt: null
check_val_every_n_epoch: 1 
val_check_interval: null 
accumulate_grad_batches: 1
gpus: '0'
seed: 41